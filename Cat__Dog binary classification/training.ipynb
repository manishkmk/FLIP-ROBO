{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "training.ipynb",
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "code",
      "metadata": {
        "id": "fxgoIsKSNBO9",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import keras\n",
        "from keras.models import Sequential\n",
        "from keras.layers import Dense,Dropout,Flatten,Activation\n",
        "from keras.layers import Conv2D,MaxPooling2D\n",
        "from keras.optimizers import SGD\n",
        "\n",
        "import pickle"
      ],
      "execution_count": 34,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "GhCxnxmumXCK",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "#This block only for access o filr using google drivr\n",
        "from pydrive.auth import GoogleAuth\n",
        "from pydrive.drive import GoogleDrive\n",
        "from google.colab import auth\n",
        "from oauth2client.client import GoogleCredentials"
      ],
      "execution_count": 2,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "_d0jAckvoeUq",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "outputId": "679ce580-dc19-46d9-e9cb-01c0ff08b533"
      },
      "source": [
        "\n",
        "import pickle\n",
        "\n",
        "pickle_in =open(r\"/content/drive/My Drive/X.pickle\",\"rb\")\n",
        "X=pickle.load(pickle_in)\n",
        "\n",
        "pickle_in =open(r\"//content/drive/My Drive/y.pickle\",\"rb\")\n",
        "y=pickle.load(pickle_in)\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "X=X/255.0\n",
        "\n",
        "print(X)"
      ],
      "execution_count": 24,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[[[[0.4745098 ]\n",
            "   [0.6       ]\n",
            "   [0.54117647]\n",
            "   ...\n",
            "   [0.5254902 ]\n",
            "   [0.54509804]\n",
            "   [0.55686275]]\n",
            "\n",
            "  [[0.4627451 ]\n",
            "   [0.60784314]\n",
            "   [0.56470588]\n",
            "   ...\n",
            "   [0.45098039]\n",
            "   [0.48235294]\n",
            "   [0.48235294]]\n",
            "\n",
            "  [[0.47843137]\n",
            "   [0.58431373]\n",
            "   [0.55686275]\n",
            "   ...\n",
            "   [0.4       ]\n",
            "   [0.38039216]\n",
            "   [0.36078431]]\n",
            "\n",
            "  ...\n",
            "\n",
            "  [[0.6745098 ]\n",
            "   [0.71372549]\n",
            "   [0.48627451]\n",
            "   ...\n",
            "   [0.18823529]\n",
            "   [0.16078431]\n",
            "   [0.16078431]]\n",
            "\n",
            "  [[0.76862745]\n",
            "   [0.76862745]\n",
            "   [0.63137255]\n",
            "   ...\n",
            "   [0.15686275]\n",
            "   [0.16862745]\n",
            "   [0.18431373]]\n",
            "\n",
            "  [[0.6       ]\n",
            "   [0.58823529]\n",
            "   [0.66666667]\n",
            "   ...\n",
            "   [0.17647059]\n",
            "   [0.18823529]\n",
            "   [0.18823529]]]\n",
            "\n",
            "\n",
            " [[[0.57254902]\n",
            "   [0.58823529]\n",
            "   [0.58039216]\n",
            "   ...\n",
            "   [0.16470588]\n",
            "   [0.18039216]\n",
            "   [0.14509804]]\n",
            "\n",
            "  [[0.58039216]\n",
            "   [0.58039216]\n",
            "   [0.61960784]\n",
            "   ...\n",
            "   [0.13333333]\n",
            "   [0.09803922]\n",
            "   [0.08235294]]\n",
            "\n",
            "  [[0.58431373]\n",
            "   [0.57647059]\n",
            "   [0.60784314]\n",
            "   ...\n",
            "   [0.25490196]\n",
            "   [0.24705882]\n",
            "   [0.27058824]]\n",
            "\n",
            "  ...\n",
            "\n",
            "  [[0.37254902]\n",
            "   [0.41568627]\n",
            "   [0.41568627]\n",
            "   ...\n",
            "   [0.2627451 ]\n",
            "   [0.26666667]\n",
            "   [0.2627451 ]]\n",
            "\n",
            "  [[0.37647059]\n",
            "   [0.39215686]\n",
            "   [0.39215686]\n",
            "   ...\n",
            "   [0.27058824]\n",
            "   [0.25098039]\n",
            "   [0.24313725]]\n",
            "\n",
            "  [[0.36862745]\n",
            "   [0.37647059]\n",
            "   [0.38039216]\n",
            "   ...\n",
            "   [0.24705882]\n",
            "   [0.22745098]\n",
            "   [0.23921569]]]\n",
            "\n",
            "\n",
            " [[[0.16078431]\n",
            "   [0.25490196]\n",
            "   [0.23921569]\n",
            "   ...\n",
            "   [0.10196078]\n",
            "   [0.07058824]\n",
            "   [0.07843137]]\n",
            "\n",
            "  [[0.2       ]\n",
            "   [0.26666667]\n",
            "   [0.3254902 ]\n",
            "   ...\n",
            "   [0.12941176]\n",
            "   [0.08627451]\n",
            "   [0.08627451]]\n",
            "\n",
            "  [[0.18039216]\n",
            "   [0.29019608]\n",
            "   [0.23921569]\n",
            "   ...\n",
            "   [0.08627451]\n",
            "   [0.09019608]\n",
            "   [0.11372549]]\n",
            "\n",
            "  ...\n",
            "\n",
            "  [[0.29803922]\n",
            "   [0.37647059]\n",
            "   [0.23529412]\n",
            "   ...\n",
            "   [0.04705882]\n",
            "   [0.05882353]\n",
            "   [0.07843137]]\n",
            "\n",
            "  [[0.14901961]\n",
            "   [0.29803922]\n",
            "   [0.3372549 ]\n",
            "   ...\n",
            "   [0.05882353]\n",
            "   [0.05490196]\n",
            "   [0.05882353]]\n",
            "\n",
            "  [[0.11764706]\n",
            "   [0.22352941]\n",
            "   [0.11372549]\n",
            "   ...\n",
            "   [0.06666667]\n",
            "   [0.05490196]\n",
            "   [0.05882353]]]\n",
            "\n",
            "\n",
            " ...\n",
            "\n",
            "\n",
            " [[[0.78431373]\n",
            "   [0.79215686]\n",
            "   [0.78039216]\n",
            "   ...\n",
            "   [0.47843137]\n",
            "   [0.54509804]\n",
            "   [0.51764706]]\n",
            "\n",
            "  [[0.81176471]\n",
            "   [0.78039216]\n",
            "   [0.76078431]\n",
            "   ...\n",
            "   [0.4745098 ]\n",
            "   [0.55686275]\n",
            "   [0.54901961]]\n",
            "\n",
            "  [[0.80392157]\n",
            "   [0.76470588]\n",
            "   [0.76862745]\n",
            "   ...\n",
            "   [0.47843137]\n",
            "   [0.62352941]\n",
            "   [0.55686275]]\n",
            "\n",
            "  ...\n",
            "\n",
            "  [[0.81960784]\n",
            "   [0.83529412]\n",
            "   [0.83921569]\n",
            "   ...\n",
            "   [0.89411765]\n",
            "   [0.89411765]\n",
            "   [0.88627451]]\n",
            "\n",
            "  [[0.81568627]\n",
            "   [0.80392157]\n",
            "   [0.83921569]\n",
            "   ...\n",
            "   [0.89019608]\n",
            "   [0.89019608]\n",
            "   [0.89019608]]\n",
            "\n",
            "  [[0.81568627]\n",
            "   [0.80392157]\n",
            "   [0.79607843]\n",
            "   ...\n",
            "   [0.89019608]\n",
            "   [0.87843137]\n",
            "   [0.85882353]]]\n",
            "\n",
            "\n",
            " [[[0.44313725]\n",
            "   [0.4627451 ]\n",
            "   [0.47058824]\n",
            "   ...\n",
            "   [0.25882353]\n",
            "   [0.27843137]\n",
            "   [0.2627451 ]]\n",
            "\n",
            "  [[0.42745098]\n",
            "   [0.45490196]\n",
            "   [0.47058824]\n",
            "   ...\n",
            "   [0.25490196]\n",
            "   [0.23529412]\n",
            "   [0.24313725]]\n",
            "\n",
            "  [[0.38823529]\n",
            "   [0.44705882]\n",
            "   [0.46666667]\n",
            "   ...\n",
            "   [0.27058824]\n",
            "   [0.25098039]\n",
            "   [0.16470588]]\n",
            "\n",
            "  ...\n",
            "\n",
            "  [[0.27058824]\n",
            "   [0.26666667]\n",
            "   [0.26666667]\n",
            "   ...\n",
            "   [0.40392157]\n",
            "   [0.29411765]\n",
            "   [0.2       ]]\n",
            "\n",
            "  [[0.2745098 ]\n",
            "   [0.28235294]\n",
            "   [0.29019608]\n",
            "   ...\n",
            "   [0.45098039]\n",
            "   [0.34509804]\n",
            "   [0.22745098]]\n",
            "\n",
            "  [[0.28235294]\n",
            "   [0.28235294]\n",
            "   [0.28627451]\n",
            "   ...\n",
            "   [0.39607843]\n",
            "   [0.32941176]\n",
            "   [0.27058824]]]\n",
            "\n",
            "\n",
            " [[[0.56862745]\n",
            "   [0.56862745]\n",
            "   [0.56862745]\n",
            "   ...\n",
            "   [0.57254902]\n",
            "   [0.56862745]\n",
            "   [0.5372549 ]]\n",
            "\n",
            "  [[0.56862745]\n",
            "   [0.56862745]\n",
            "   [0.56862745]\n",
            "   ...\n",
            "   [0.54901961]\n",
            "   [0.53333333]\n",
            "   [0.50980392]]\n",
            "\n",
            "  [[0.57647059]\n",
            "   [0.57254902]\n",
            "   [0.56862745]\n",
            "   ...\n",
            "   [0.55294118]\n",
            "   [0.50196078]\n",
            "   [0.4627451 ]]\n",
            "\n",
            "  ...\n",
            "\n",
            "  [[0.31372549]\n",
            "   [0.31764706]\n",
            "   [0.31372549]\n",
            "   ...\n",
            "   [0.38823529]\n",
            "   [0.38431373]\n",
            "   [0.38039216]]\n",
            "\n",
            "  [[0.31764706]\n",
            "   [0.31764706]\n",
            "   [0.31372549]\n",
            "   ...\n",
            "   [0.37647059]\n",
            "   [0.37647059]\n",
            "   [0.36862745]]\n",
            "\n",
            "  [[0.30980392]\n",
            "   [0.30196078]\n",
            "   [0.29803922]\n",
            "   ...\n",
            "   [0.36470588]\n",
            "   [0.35686275]\n",
            "   [0.36078431]]]]\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "HtPug_b1vz1d",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "w5D3yrtEneYy",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "outputId": "58dc5ee1-462f-4597-e410-d5b05f7acb84"
      },
      "source": [
        "#first convolution extracts 32 filters that are 3x3\n",
        "#Convolution is followed by max-pooling layer with a 2x2 windows\n",
        "model = Sequential()\n",
        "model.add(Conv2D(32, (3, 3), activation='relu', kernel_initializer='he_uniform', padding='same',input_shape=X.shape[1:]))\n",
        "model.add(MaxPooling2D((2, 2)))\n",
        "model.add(Dropout(0.2))\n",
        "\n",
        "\n",
        "#Second convolution extracts 32 filters that are 3x3\n",
        "#Convolution is followed by max-pooling layer with a 2x2 windows\n",
        "model.add(Conv2D(64, (3, 3), activation='relu', kernel_initializer='he_uniform', padding='same'))\n",
        "model.add(MaxPooling2D((2, 2)))\n",
        "model.add(Dropout(0.2))\n",
        "\n",
        "#Third Convolution extract 128 filters that are 3x3\n",
        "#Convolution is followed by max-pooling layers with 2x2 windows\n",
        "model.add(Conv2D(128, (3, 3), activation='relu', kernel_initializer='he_uniform', padding='same'))\n",
        "model.add(MaxPooling2D((2, 2)))\n",
        "model.add(Dropout(0.2))\n",
        "\n",
        "#Flatten layers amp to a 1-dim tensor so we can add fully connected layers\n",
        "model.add(Flatten())\n",
        "\n",
        "#Creat a fully connected layers with Relu activation and 128 hidden layers\n",
        "model.add(Dense(128, activation='relu', kernel_initializer='he_uniform'))\n",
        "model.add(Dropout(0.5))\n",
        "\n",
        "#create a output layers with two nodes and sigmoid activation\n",
        "model.add(Dense(1, activation='sigmoid'))\n",
        "# compile model\n",
        "opt = SGD(lr=0.001, momentum=0.9)\n",
        "model.compile(optimizer=opt, loss='binary_crossentropy', metrics=['accuracy'])\n",
        "  \n",
        "model.fit(X,y,batch_size=15,epochs=40,validation_split=0.3)\n",
        "  \n"
      ],
      "execution_count": 38,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Train on 17442 samples, validate on 7476 samples\n",
            "Epoch 1/40\n",
            "17442/17442 [==============================] - 17s 999us/step - loss: 0.6978 - accuracy: 0.5294 - val_loss: 0.6879 - val_accuracy: 0.5805\n",
            "Epoch 2/40\n",
            "17442/17442 [==============================] - 17s 999us/step - loss: 0.6791 - accuracy: 0.5666 - val_loss: 0.6784 - val_accuracy: 0.5726\n",
            "Epoch 3/40\n",
            "17442/17442 [==============================] - 17s 976us/step - loss: 0.6588 - accuracy: 0.6047 - val_loss: 0.6511 - val_accuracy: 0.6394\n",
            "Epoch 4/40\n",
            "17442/17442 [==============================] - 17s 979us/step - loss: 0.6360 - accuracy: 0.6364 - val_loss: 0.6172 - val_accuracy: 0.6734\n",
            "Epoch 5/40\n",
            "17442/17442 [==============================] - 17s 980us/step - loss: 0.6101 - accuracy: 0.6643 - val_loss: 0.5930 - val_accuracy: 0.7021\n",
            "Epoch 6/40\n",
            "17442/17442 [==============================] - 17s 978us/step - loss: 0.5859 - accuracy: 0.6935 - val_loss: 0.5798 - val_accuracy: 0.6932\n",
            "Epoch 7/40\n",
            "17442/17442 [==============================] - 17s 987us/step - loss: 0.5560 - accuracy: 0.7184 - val_loss: 0.5389 - val_accuracy: 0.7382\n",
            "Epoch 8/40\n",
            "17442/17442 [==============================] - 17s 976us/step - loss: 0.5313 - accuracy: 0.7362 - val_loss: 0.5307 - val_accuracy: 0.7334\n",
            "Epoch 9/40\n",
            "17442/17442 [==============================] - 17s 973us/step - loss: 0.5129 - accuracy: 0.7505 - val_loss: 0.5112 - val_accuracy: 0.7442\n",
            "Epoch 10/40\n",
            "17442/17442 [==============================] - 17s 975us/step - loss: 0.4959 - accuracy: 0.7609 - val_loss: 0.4961 - val_accuracy: 0.7661\n",
            "Epoch 11/40\n",
            "17442/17442 [==============================] - 17s 979us/step - loss: 0.4742 - accuracy: 0.7736 - val_loss: 0.4897 - val_accuracy: 0.7590\n",
            "Epoch 12/40\n",
            "17442/17442 [==============================] - 17s 996us/step - loss: 0.4597 - accuracy: 0.7834 - val_loss: 0.4908 - val_accuracy: 0.7568\n",
            "Epoch 13/40\n",
            "17442/17442 [==============================] - 17s 1000us/step - loss: 0.4483 - accuracy: 0.7895 - val_loss: 0.4748 - val_accuracy: 0.7628\n",
            "Epoch 14/40\n",
            "17442/17442 [==============================] - 17s 995us/step - loss: 0.4311 - accuracy: 0.8002 - val_loss: 0.4315 - val_accuracy: 0.8046\n",
            "Epoch 15/40\n",
            "17442/17442 [==============================] - 17s 983us/step - loss: 0.4165 - accuracy: 0.8076 - val_loss: 0.4203 - val_accuracy: 0.8083\n",
            "Epoch 16/40\n",
            "17442/17442 [==============================] - 17s 983us/step - loss: 0.4050 - accuracy: 0.8154 - val_loss: 0.4562 - val_accuracy: 0.7813\n",
            "Epoch 17/40\n",
            "17442/17442 [==============================] - 17s 983us/step - loss: 0.3961 - accuracy: 0.8224 - val_loss: 0.4215 - val_accuracy: 0.8056\n",
            "Epoch 18/40\n",
            "17442/17442 [==============================] - 17s 980us/step - loss: 0.3830 - accuracy: 0.8274 - val_loss: 0.4010 - val_accuracy: 0.8173\n",
            "Epoch 19/40\n",
            "17442/17442 [==============================] - 17s 983us/step - loss: 0.3680 - accuracy: 0.8375 - val_loss: 0.4155 - val_accuracy: 0.8068\n",
            "Epoch 20/40\n",
            "17442/17442 [==============================] - 17s 997us/step - loss: 0.3563 - accuracy: 0.8401 - val_loss: 0.4207 - val_accuracy: 0.8062\n",
            "Epoch 21/40\n",
            "17442/17442 [==============================] - 17s 975us/step - loss: 0.3441 - accuracy: 0.8464 - val_loss: 0.4227 - val_accuracy: 0.8091\n",
            "Epoch 22/40\n",
            "17442/17442 [==============================] - 17s 977us/step - loss: 0.3327 - accuracy: 0.8545 - val_loss: 0.3795 - val_accuracy: 0.8305\n",
            "Epoch 23/40\n",
            "17442/17442 [==============================] - 17s 971us/step - loss: 0.3210 - accuracy: 0.8611 - val_loss: 0.3773 - val_accuracy: 0.8319\n",
            "Epoch 24/40\n",
            "17442/17442 [==============================] - 17s 967us/step - loss: 0.3107 - accuracy: 0.8612 - val_loss: 0.3941 - val_accuracy: 0.8204\n",
            "Epoch 25/40\n",
            "17442/17442 [==============================] - 17s 965us/step - loss: 0.3010 - accuracy: 0.8694 - val_loss: 0.4268 - val_accuracy: 0.8091\n",
            "Epoch 26/40\n",
            "17442/17442 [==============================] - 17s 976us/step - loss: 0.2873 - accuracy: 0.8762 - val_loss: 0.3743 - val_accuracy: 0.8343\n",
            "Epoch 27/40\n",
            "17442/17442 [==============================] - 17s 972us/step - loss: 0.2793 - accuracy: 0.8788 - val_loss: 0.3673 - val_accuracy: 0.8377\n",
            "Epoch 28/40\n",
            "17442/17442 [==============================] - 17s 974us/step - loss: 0.2697 - accuracy: 0.8841 - val_loss: 0.3803 - val_accuracy: 0.8353\n",
            "Epoch 29/40\n",
            "17442/17442 [==============================] - 17s 982us/step - loss: 0.2583 - accuracy: 0.8919 - val_loss: 0.3756 - val_accuracy: 0.8340\n",
            "Epoch 30/40\n",
            "17442/17442 [==============================] - 17s 992us/step - loss: 0.2486 - accuracy: 0.8970 - val_loss: 0.3713 - val_accuracy: 0.8403\n",
            "Epoch 31/40\n",
            "17442/17442 [==============================] - 17s 997us/step - loss: 0.2350 - accuracy: 0.9010 - val_loss: 0.4073 - val_accuracy: 0.8280\n",
            "Epoch 32/40\n",
            "17442/17442 [==============================] - 18s 1ms/step - loss: 0.2258 - accuracy: 0.9073 - val_loss: 0.3951 - val_accuracy: 0.8355\n",
            "Epoch 33/40\n",
            "17442/17442 [==============================] - 18s 1ms/step - loss: 0.2131 - accuracy: 0.9119 - val_loss: 0.4305 - val_accuracy: 0.8277\n",
            "Epoch 34/40\n",
            "17442/17442 [==============================] - 18s 1ms/step - loss: 0.1998 - accuracy: 0.9174 - val_loss: 0.4200 - val_accuracy: 0.8328\n",
            "Epoch 35/40\n",
            "17442/17442 [==============================] - 18s 1ms/step - loss: 0.2003 - accuracy: 0.9174 - val_loss: 0.3996 - val_accuracy: 0.8398\n",
            "Epoch 36/40\n",
            "17442/17442 [==============================] - 18s 1ms/step - loss: 0.1803 - accuracy: 0.9247 - val_loss: 0.3981 - val_accuracy: 0.8426\n",
            "Epoch 37/40\n",
            "17442/17442 [==============================] - 18s 1ms/step - loss: 0.1763 - accuracy: 0.9293 - val_loss: 0.4495 - val_accuracy: 0.8268\n",
            "Epoch 38/40\n",
            "17442/17442 [==============================] - 18s 1ms/step - loss: 0.1711 - accuracy: 0.9317 - val_loss: 0.4703 - val_accuracy: 0.8301\n",
            "Epoch 39/40\n",
            "17442/17442 [==============================] - 18s 1ms/step - loss: 0.1596 - accuracy: 0.9360 - val_loss: 0.4334 - val_accuracy: 0.8455\n",
            "Epoch 40/40\n",
            "17442/17442 [==============================] - 18s 1ms/step - loss: 0.1528 - accuracy: 0.9404 - val_loss: 0.4101 - val_accuracy: 0.8443\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<keras.callbacks.callbacks.History at 0x7f9f6a2f4630>"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 38
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "XuwvWeucm86n",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "model.save(r\"D:\\final binary classification\\Dogs_vs_Cats_CNN.model\")"
      ],
      "execution_count": 41,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ZsP0cfQMm5R7",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}